

HISTOIRE


Les premières traces de l’IA remontent à 1950 dans un article d’Alan Turing intitulé “Computing Machinery and Intelligence” dans lequel le mathématicien explore le problème de définir si une machine est consciente ou non. De cet article découlera ce que l’on appelle aujourd’hui le Test de Turing qui permet d’évaluer la capacité d’une machine à tenir une conversation humaine.

Cependant l’officialisation de l’intelligence artificielle comme véritable domaine scientifique date de 1956 lors d’une conférence aux États-Unis qui s’est tenue au Dartmouth College. Par la suite, ce domaine atteindra de prestigieuses universités comme celles de Stanford, du MIT, ou encore d’Édimbourg.

Dès le milieu des années 60-80, la recherche autour de l’IA sur le sol américain était principalement financée par le Département de la Défense. Dans le même temps, des laboratoires ouvrent çà et là à travers le monde. Certains experts prédisaient à l’époque que « des machines seront capables, d’ici 20 ans, de faire le travail que toute personne peut faire ». Si l’idée était visionnaire, même en 2018 l’intelligence artificielle n’a pas encore pris cette importance dans nos vies.
L'approche dite "symbolique" (déduction, classification, hiérarchisation…), a permis le développement d’outils capables de reproduire les mécanismes cognitifs d'un expert. C’est pourquoi on les a baptisés « systèmes experts ». 
L’approche dite "numérique" raisonne sur les données. Le système cherche des régularités dans les données disponibles pour extraire des connaissances, sans modèle préétabli. Cette méthode, née avec entre autres les réseaux de neurones artificiels dans les années 1980, se développe aujourd’hui grâce à l’augmentation de puissance des ordinateurs et à l’accumulation des gigantesques quantités de données, le fameux big data.


Dans les années 80, le succès des systèmes experts permet de relancer les projets de recherche sur l’intelligence artificielle. Un système expert était un ordinateur capable de se comporter comme un expert (humain), mais dans un domaine bien précis. Grâce à ce succès, le marché de l’IA atteint une valeur d’un milliard de dollars, ce qui motive les différents gouvernements à de nouveau soutenir financièrement plus de projets académiques.

Le développement exponentiel des performances informatiques permet entre 1990 et 2000 d’exploiter l’IA sur des terrains jusqu’alors peu communs. On retrouve à cette époque le data mining(analyses prédictives et de l'exploitation des données), ou encore les diagnostics médicaux. Il faudra attendre 1997 pour une véritable sortie médiatique lorsque le fameux Deep Blue créé par IBM a battu Garry Kasparov, alors champion du monde d’échec.

Entre 2000 et 2010, notre société vit un véritable boom informatique.
-Ethique de l'intégration de L'IA dans de nombreux secteurs

l'évolution de la puissance de calcul des machines dans les années 2010 a impacté l'IA. qui est dès lors devenue un enjeu majeur, amenant des débats sur les risques futurd d'une IA devenue trop autonome ou consciente ainsi que le déséquilibre lié à 'emploi. Cela impacte également l'innovation humaine car les capacités algorithmiques et sa puissance de traitement informatique surpassent son équivalent biologique.

L’intelligence artificielle représente dans certains secteurs une bonne partie du futur de la civilisation humaine. Pour exemple, les remèdes contre le cancer ou les solutions pour la sécurité et la sûreté reposent tous sur ce secteur de la technologie.




« L’intelligence artificielle est l’une des choses les plus importantes sur laquelle travaille l’Homme », c’est encore plus important que l’électricité ou le feu, nous voulons la développer de manière réfléchie. L’intelligence artificielle a le potentiel de faire les plus grandes avancées dont nous serons témoins, comme guérir le cancer par exemple. Donc nous lui devons de faire nous aussi des progrès ».
Sundar Pichai( président-directeur général de Google depuis le 10 août 2015)



PROCESS INTRO

Derrière l'intelligence artificielle, se cache une multitude d'appareils qui ont un dénominateur commun : le machine learning. Cette technologie permet de stocker une grande quantité de données dans un cerveau ou réseau neuronal virtuel. 

 Le champs applicatif de l'IA ne cesse de grandir, notemment en medecine avec les opérations assistées, le suivi des patients à distance, les prothèses intelligentes, les traitements personnalisés grâce au recoupement d’un nombre croissant de données (big data). Les chercheurs développent pour cela des approches et techniques multiples, du traitement des langues,  à la fouille de données et à l’apprentissage automatique. 
 
Les tenants de l’intelligence artificielle dite forte visent à concevoir une machine capable de raisonner comme l’humain, avec le risque supposé de générer une machine supérieure à l’homme et dotée d’une conscience propre. Cette voie est toujours en cours d'exploration.

D’un autre côté, les tenants de l’intelligence artificielle dite faible mettent en œuvre toutes les technologies disponibles pour concevoir des machines capables d’aider les humains dans leurs tâches. Ce champ de recherche mobilise de nombreuses disciplines, de l’informatique aux sciences cognitives en passant par les mathématiques, sans oublier les connaissances spécialisées des domaines auxquels on souhaite l’appliquer.

Les machines dotées d'une intelligence artificielle mémorisent des comportements. Ce travail de mémorisation leur permet par la suite de résoudre des problèmes, et d'agir correctement face à telle ou telle situation. Cet apprentissage se réalise à l'aide de bases de données et d'algorithmes. Ce travail complexe aide la machine à mesurer l'importance d'un problème, à passer au crible les solutions possibles et les situations passées similaires afin de bien agir.


bleu-clair-titre: #00C4F5
noir-violet: #20172A
violet-pourpre: #471A39
magenta: #9F1F72
violet: #3E254C
bleu-foncé: #2C517B
